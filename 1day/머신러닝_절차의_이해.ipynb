{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E85W4AGWEyYi"
      },
      "source": [
        "# 머신러닝 절차의 이해"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7RSfXhcEyYl"
      },
      "source": [
        "1. 머신러닝의 절차  \n",
        "  \n",
        "문제 정의 => 데이터 수집 => 데이터 전처리 => 데이터 학습 => 모델 평가  \n",
        "   \n",
        "1) 문제 정의  \n",
        "문제가 무엇이고, 이를 해결하기 위한 데이터는 무엇인지에 대한 정의  \n",
        "- 분류와 회귀 문제\n",
        "머신 러닝의 많은 문제는 분류 또는 회귀 문제에 속한다. 회귀는 입력 데이터를 통해 하나의 값을 수렴하는 문제이고 분류는 입력 데이터를 이진 또는 다중 클래스로 분류하는 문제다.  \n",
        "  \n",
        "  \n",
        "- 지도학습과 비지도 학습  \n",
        "지도 학습이란 레이블(Label)이라는 정답과 함께 학습하는 것  \n",
        "비지도 학습은 기본적으로 목적 데이터(또는 레이블)이 없는 학습 방법이다. 군집(clustering)이나 차원 축소와 같은 학습 방법들을 비지도 학습에 속한다.    \n",
        "  \n",
        "2) 데이터 수집  \n",
        "학습에 필요한 데이터를 수집한다. 공공데이터를 활용할 수도 있고 데이터를 제공하는 기관을 검색하여 수집하거나 직접 만들어야 하는 경우도 있다.  \n",
        "\n",
        "3) 데이터 전처리  \n",
        "수집한 데이터를 학습에 바로 사용하면 프로그램이 중단되거나 학습률이 떨어질 수 있다. 그래서 수집한 데이터에 대해 전처리를 하는 것이 좋다. 누락된 데이터를 제거하거나 기본 값으로 채우는 것은 기본이다.   \n",
        "  \n",
        "학습 데이터 중 다른 데이터에 비해 너무 동떨어진 데이터들(이상치)이 있는데 이것들은 제거하고 학습 시키는 것이 학습효과를 높일 수 있다. 이를 특이점 제거라고 한다. \n",
        "   \n",
        "데이터를 머신러닝에 적합한 형태로 변환하면 학습 시간을 단축시킬 수 있는데 이러한 방법에는 표준화, 정규화, 이산화 등이 있다.  \n",
        "   \n",
        "데이터 축소는 데이터 속성중 학습 영향력이 작은 속성을 제거하는 방법을 말하는데 이 속성 때문에 학습의 신뢰성이 떨어질 수 있기 때문이다.    \n",
        "  \n",
        "4) 데이터 학습  \n",
        "학습 데이터가 준비된 다음 절차는 학습이다. 이 때 이 문제 해결을 위한 적절한 학습 방법이나 알고리즘을 선택해야 한다. 학습방법을 지도학습으로 할지 비지도 학습을 할지, 알고리즘을 회귀, 분류, 의사결정트리, SVM 등 다양한 방법 중 어떤 것으로 할지를 선택하여 모델을 생성하고 학습을 진행한다.   \n",
        "  \n",
        "5) 모델 평가  \n",
        "학습이 완료되면 학습하지 않은 데이터를 가지고 테스팅을 하여 정확도를 측정한다. 정확도가 좋지 않다면 하이퍼파라메터나 데이터를 조절하여 다시 학습을 수행하고 테스팅과정을 반복한다.  \n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0gelzOaEyYl"
      },
      "source": [
        "# 사이킷런을 이용한 머신러닝 절차 실습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oHRwF15OEyYm"
      },
      "source": [
        "<데이터 로드>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rkOL2TWvEyYm"
      },
      "source": [
        " \n",
        "  \n",
        "- 샘플과 특성  \n",
        "많은 머신 러닝 문제가 1개 이상의 독립 변수 x를 가지고 종속 변수 y를 예측하는 문제다. 그래서 많은 머신러닝 모델들이 독립 변수, 종속 변수, 가중치, 편향 등을 행렬 연산을 통해 연산하는 경우가 많다. 독립 변수 x의 행렬을 X라고 하였을 때, 독립 변수의 개수가 n개이고 데이터의 개수가 m인 행렬 X는 다음과 같다.  \n",
        "  \n",
        "![5-2.jpg](attachment:5-2.jpg)   \n",
        "  \n",
        "- 타겟 값(레이블)  \n",
        "회귀의 경유엔 실제 값, 분류의 경우엔 분류 클래스를 표현하는 정수가 된다. 비지도 학습은 타겟이 없다.  \n",
        "보통 샘플 한 줄에 타겟 값 하나가 매칭되는 1차원 배열형태로 제공됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wGKnJyEEyYn"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u84k0nu5EyYo",
        "outputId": "58a266f2-cd9e-4f5c-c9ea-bab31390983a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iris target값: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "iris target명: ['setosa' 'versicolor' 'virginica']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "\n",
              "   label  \n",
              "0      0  \n",
              "1      0  \n",
              "2      0  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# 붓꽃 데이터 세트를 로딩\n",
        "iris = load_iris()\n",
        "\n",
        "# iris.data는 Iris 데이터 세트에서 피처(feature)만으로 된 데이터 numpy\n",
        "iris_data = iris.data\n",
        "\n",
        "# iris.target은 붓꽃 데이터 세트에서 레이블(결정 값) numpy \n",
        "iris_label = iris.target\n",
        "print('iris target값:', iris_label)\n",
        "print('iris target명:', iris.target_names)\n",
        "\n",
        "# 붓꽃 데이터 세트를 DataFrame으로 변환 \n",
        "iris_df = pd.DataFrame(data=iris_data, columns=iris.feature_names)\n",
        "iris_df['label'] = iris.target\n",
        "iris_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLLWhe2nEyYp"
      },
      "source": [
        "<데이터 전처리>  \n",
        "  \n",
        "보통 머신러닝 학습전에 데이터에 대한 전처리가 필요한데, 사이킷 런에서 제공하는 데이터들은 따로 이 작업이 필요없어 그냥 학습하는 경우가 많다.  \n",
        "하지만 실제 사용되는 데이터는 학습에 꼭 맞게 정형화되어 있지 않으므로 학습에 맞게 데이터 전처리가 필요한 경우가 많다.  \n",
        "사이킷 런은 이러한 작업을 쉽게 하도록 다양한 데이터 전처리 함수를 제공한다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGlNiAmkEyYp",
        "outputId": "c75b16bf-fd01-4238-bb31-51d2de0d7c63"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([0.5, 2.5])"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "X = [[0, 15],\n",
        "     [1, -10]]\n",
        "# scale data according to computed scaling values\n",
        "std_scaler=StandardScaler()\n",
        "fitX = std_scaler.fit(X)\n",
        "fitX.mean_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "On1XluoOEyYq",
        "outputId": "45f836f1-15f7-4b72-d642-20600a5ce447"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[-1.,  1.],\n",
              "       [ 1., -1.]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "fitX.transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gq7PLbtdEyYq"
      },
      "source": [
        "<전체 데이터를 학습 데이터와 테스트 데이터로 분리>  \n",
        "  \n",
        "학습한 데이터로 테스팅하면 문제를 이미 알고 있는 상태로 시험보는 것과 마찬가지이므로  \n",
        "학습데이터와 테스팅 데이터를 분리한다.  \n",
        "  \n",
        "- train_test_split(학습데이터, 타겟, 테스팅 데이터 비율, random_state)  \n",
        "  \n",
        "random_state: 호출할 때마다 같은 학습/테스트 용 데이터 세트를 생성하기 위해 주어지는 난수 발생 값.  \n",
        "random_state를 지정하지 않으면 수행할 때마다 다른 학습/테스트 용 데이터를 만들 수 있다.  \n",
        "동일한 데이터 세트로 분리하기 위해 random_state를 일정한 숫자 값으로 부여"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MazSNzNJEyYq"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(iris_data, iris_label, \n",
        "                                                    test_size=0.2, random_state=11)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "giJANA6sEyYr"
      },
      "source": [
        "<원하는 알고리즘 모델 객체를 생성하여 학습 수행>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JLk0ke7YEyYr",
        "outputId": "6b7f425f-9a65-40b9-b5ab-1d580f4919e3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DecisionTreeClassifier(random_state=11)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# DecisionTreeClassifier(의사결정트리) 객체 생성 \n",
        "dt_clf = DecisionTreeClassifier(random_state=11)\n",
        "\n",
        "# 학습 수행 \n",
        "dt_clf.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_tz1r4lEyYs"
      },
      "source": [
        "<예측>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hgrO8pcJEyYs"
      },
      "outputs": [],
      "source": [
        "pred = dt_clf.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xl7XDxXJEyYs",
        "outputId": "248a511c-bfbc-4349-aa45-479cc74e1832"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 2, 1, 1, 2, 0, 1, 0, 0, 1, 1, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0,\n",
              "       0, 1, 0, 0, 2, 1, 0, 1])"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 머신이 학습한 뒤 시험본 답안\n",
        "pred "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfl0mEhpEyYt",
        "outputId": "0145534d-513c-44a2-dc1f-792b91cf3d6b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([2, 2, 2, 1, 2, 0, 1, 0, 0, 1, 2, 1, 1, 2, 2, 0, 2, 1, 2, 2, 1, 0,\n",
              "       0, 1, 0, 0, 2, 1, 0, 1])"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# 정답\n",
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5DhMCHBzEyYt"
      },
      "source": [
        "<평가>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob5s0OdZEyYt",
        "outputId": "ea7145eb-0652-4dc8-dcd6-540eec0db3cd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "예측 정확도: 0.9333\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0PTAIyKEyYu"
      },
      "source": [
        "<파이프라인>  \n",
        "  \n",
        "사이킷 런은 학습과 데이터전처리를 파이프라인으로 묶어서 처리할 수 있는 방법을 제공한다. \n",
        "파이프라인 사용을 권장하는 이유는 특정 데이터에 대해서 정규화처리를 하고 검증, 테스팅을 수행하면 데이터의 격차가 커 학습의 의미가 없어진다.  \n",
        "그래서 모든 데이터에 대해서 따로 처리하는 것보다 파이프라인으로 묶어서 처리하면 이러한 문제를 예방할 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpNOJBgfEyYu",
        "outputId": "48055293-1d52-4127-fc45-0c55f42ef44f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.9736842105263158"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 파이프 객체 생성\n",
        "pipe = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    LogisticRegression()\n",
        ")\n",
        "\n",
        "# 학습 데이터 로드\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# 생성한 파이프 세트로 학습\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "\n",
        "# 예측 및 평가\n",
        "accuracy_score(pipe.predict(X_test), y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2Xx8_deEyYu"
      },
      "source": [
        "학습/테스트 데이터 세트 분리  \n",
        "train_test_split(학습데이터, 정답, test_size=테스트데이터비율, random_state=11)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qej0iwkyEyYv",
        "outputId": "21fd8abb-9cee-4a9d-bec1-eb50c945bf11"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "예측 정확도: 1.0\n"
          ]
        }
      ],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "dt_clf = DecisionTreeClassifier()\n",
        "train_data = iris.data\n",
        "train_label = iris.target\n",
        "dt_clf.fit(train_data, train_label)\n",
        "\n",
        "# 학습 데이터 셋으로 예측 수행\n",
        "pred = dt_clf.predict(train_data)\n",
        "print('예측 정확도:',accuracy_score(train_label,pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wrgRbryEyYv"
      },
      "source": [
        "학습한 데이터를 그대로 테스트하여 정확도 100%가 나옴.  \n",
        "아래 코드는 학습 데이터와 테스트 데이터 분리하여 학습 및 테스트 수행  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23Wjo9yoEyYw",
        "outputId": "b385605e-ebc0-4e4c-e6a3-18c7fce1dfe7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "예측 정확도: 0.9556\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "dt_clf = DecisionTreeClassifier( )\n",
        "iris_data = load_iris()\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, \n",
        "                                                    test_size=0.3, random_state=121)\n",
        "dt_clf.fit(X_train, y_train)\n",
        "pred = dt_clf.predict(X_test)\n",
        "print('예측 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfN8gIlqEyYw"
      },
      "source": [
        "<모델의 평가>    \n",
        "데이터는 모델을 평가하기 위해서 데이터를 훈련용, 검증용, 테스트용 이렇게 세 가지로 분리하여 사용한다.  \n",
        "  \n",
        "![5-1.jpg](attachment:5-1.jpg)\n",
        "  \n",
        "검증용 데이터는 모델의 성능을 평가하기 위한 용도가 아니라, 모델의 성능을 조정하기 위한 용도다. 더 정확히는 과적합이 되고 있는지 판단하거나 하이퍼파라미터의 조정을 위한 용도다. 하이퍼파라미터(초매개변수)란 값에 따라서 모델의 성능에 영향을 주는 매개변수들을 말한다. 반면, 가중치와 편향과 같은 학습을 통해 바뀌어져가는 변수를 매개변수라 한다.   \n",
        "  \n",
        "하이퍼파라미터와 매개변수의 가장 큰 차이는 하이퍼파라미터는 보통 사용자가 직접 정해줄 수 있는 변수라는 점이다. 학습률(learning rate)이나 딥 러닝의 은닉층의 수, 뉴런의 수, 드롭아웃 비율 등이 이에 해당된다. 반면 매개변수는 사용자가 결정해주는 값이 아니라 모델이 학습하는 과정에서 얻어지는 값이다. 정리하면 하이퍼파라미터는 사람이 정하는 변수인 반면, 매개변수는 기계가 훈련을 통해서 바꾸는 변수라고 할 수 있다.\n",
        "  \n",
        "훈련용 데이터로 훈련을 모두 시킨 모델은 검증용 데이터를 사용하여 정확도를 검증하며 하이퍼파라미터를 튜닝(tuning)한다. 하이퍼파라미터 튜닝이 끝났다면, 이제 검증용 데이터로 모델을 평가하는 것은 적합하지 않다. 모델은 검증용 데이터에 대해서도 일정 부분 최적화가 되어있기 때문이다. 모델에 대한 평가는 모델이 아직까지 보지 못한 데이터로 하는 것이 가장 바람직하다. 검증이 끝났다면 테스트 데이터를 가지고 모델의 진짜 성능을 평가한다.  \n",
        "  \n",
        "* 과적합(Overfitting)과 과소 적합(Underfitting)   \n",
        "  \n",
        "과적합(Overfitting)이란 훈련 데이터를 과하게 학습한 경우를 말한다.  기계가 훈련 데이터에 대해서만 과하게 학습하면 테스트 데이터나 실제 서비스에서의 데이터에 대해서는 정확도가 좋지 않은 현상이 발생한다. 이를 과적합이라 한다. 과적합 상황에서는 훈련 데이터에 대해서는 오차가 낮지만, 테스트 데이터에 대해서는 오차가 높아지는 상황이 발생한다.  그래서 테스트 데이터의 오차가 증가하기 전이나, 정확도가 감소하기 전에 훈련을 멈추는 것이 바람직하다.\n",
        "   \n",
        "과소적합(Underfitting)은 테스트 데이터의 성능이 올라갈 여지가 있음에도 훈련을 덜 한 상태를 말하는 것으로 과대 적합과는 달리 훈련 데이터에 대해서도 보통 정확도가 낮다는 특징이 있다.   \n",
        "\n",
        "\n",
        "< 교차검증 >  \n",
        "  \n",
        "1) k 폴드 교차검증    \n",
        "위의 예처럼 학습과 테스트 데이터를 분리했다고 하여 과적합을 방지할 수는 없다. 이러한 문제를 해결하기 위해 교차검증을 사용한다. 교차검증은 전체 데이터셋을 일정크기로 나눈다. 예를 들어 전체 데이터를 5개 그룹으로 나누고 이 중 4개의 그룹은 학습데이터, 1개 그룹을 검증 데이터로 사용하는데 에포크마다 학습데이터와 검증 데이터 배치를 변경하여 실행한다. 이를 그룹의 수만큼 반복한다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dLQdap27EyYw",
        "outputId": "560f5ade-a8ac-411c-a175-d68f37d4227f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "붓꽃 데이터 세트 크기: 150\n",
            "\n",
            "#1 교차 검증 정확도 :1.0, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
            "#1 검증 세트 인덱스:[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
            " 24 25 26 27 28 29]\n",
            "\n",
            "#2 교차 검증 정확도 :0.9667, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
            "#2 검증 세트 인덱스:[30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53\n",
            " 54 55 56 57 58 59]\n",
            "\n",
            "#3 교차 검증 정확도 :0.8667, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
            "#3 검증 세트 인덱스:[60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83\n",
            " 84 85 86 87 88 89]\n",
            "\n",
            "#4 교차 검증 정확도 :0.9333, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
            "#4 검증 세트 인덱스:[ 90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
            " 108 109 110 111 112 113 114 115 116 117 118 119]\n",
            "\n",
            "#5 교차 검증 정확도 :0.7333, 학습 데이터 크기: 120, 검증 데이터 크기: 30\n",
            "#5 검증 세트 인덱스:[120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
            " 138 139 140 141 142 143 144 145 146 147 148 149]\n",
            "\n",
            "## 평균 검증 정확도: 0.9\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import KFold\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "features = iris.data\n",
        "label = iris.target\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "# 5개의 폴드 세트로 분리하는 KFold 객체와 폴드 세트별 정확도를 담을 리스트 객체 생성.\n",
        "kfold = KFold(n_splits=5)\n",
        "cv_accuracy = []\n",
        "print('붓꽃 데이터 세트 크기:',features.shape[0])\n",
        "n_iter = 0\n",
        "\n",
        "# KFold객체의 split( ) 호출하면 폴드 별 학습용, 검증용 테스트의 로우 인덱스를 array로 반환  \n",
        "for train_index, test_index  in kfold.split(features):\n",
        "    # kfold.split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = label[train_index], label[test_index]\n",
        "    #학습 및 예측 \n",
        "    dt_clf.fit(X_train , y_train)    \n",
        "    pred = dt_clf.predict(X_test)\n",
        "    n_iter += 1\n",
        "    # 반복 시 마다 정확도 측정 \n",
        "    accuracy = np.round(accuracy_score(y_test,pred), 4)\n",
        "    train_size = X_train.shape[0]\n",
        "    test_size = X_test.shape[0]\n",
        "    print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'\n",
        "          .format(n_iter, accuracy, train_size, test_size))\n",
        "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
        "    cv_accuracy.append(accuracy)\n",
        "    \n",
        "# 개별 iteration별 정확도를 합하여 평균 정확도 계산 \n",
        "print('\\n## 평균 검증 정확도:', np.mean(cv_accuracy)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWQ6zg_nEyYx"
      },
      "source": [
        "Stratified K 폴드  \n",
        "학습 레이블의 분포가 너무 치우처 문제가 될 경우 이를 균등 분포 되도록 배치하여 학습해야 한다. 이렇게 레이블을 균등분포 되도록 k폴드를 만드는 방법이다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsgfmpzQEyYx",
        "outputId": "70c6cd3c-6ad8-47fd-f648-013f001dc48d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## 교차 검증: 1\n",
            "학습 레이블 데이터 분포:\n",
            " 2    50\n",
            "1    50\n",
            "Name: label, dtype: int64\n",
            "검증 레이블 데이터 분포:\n",
            " 0    50\n",
            "Name: label, dtype: int64\n",
            "## 교차 검증: 2\n",
            "학습 레이블 데이터 분포:\n",
            " 2    50\n",
            "0    50\n",
            "Name: label, dtype: int64\n",
            "검증 레이블 데이터 분포:\n",
            " 1    50\n",
            "Name: label, dtype: int64\n",
            "## 교차 검증: 3\n",
            "학습 레이블 데이터 분포:\n",
            " 1    50\n",
            "0    50\n",
            "Name: label, dtype: int64\n",
            "검증 레이블 데이터 분포:\n",
            " 2    50\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "iris_df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
        "iris_df['label']=iris.target\n",
        "iris_df['label'].value_counts()\n",
        "\n",
        "kfold = KFold(n_splits=3)\n",
        "# kfold.split(X)는 폴드 세트를 3번 반복할 때마다 달라지는 학습/테스트 용 데이터 로우 인덱스 번호 반환. \n",
        "n_iter =0\n",
        "for train_index, test_index  in kfold.split(iris_df):\n",
        "    n_iter += 1\n",
        "    label_train= iris_df['label'].iloc[train_index]\n",
        "    label_test= iris_df['label'].iloc[test_index]\n",
        "    print('## 교차 검증: {0}'.format(n_iter))\n",
        "    print('학습 레이블 데이터 분포:\\n', label_train.value_counts())\n",
        "    print('검증 레이블 데이터 분포:\\n', label_test.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHTB2ZHZEyYx",
        "outputId": "fa91c465-11b4-41d5-a874-c1b7b03bf42b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "## 교차 검증: 1\n",
            "학습 레이블 데이터 분포:\n",
            " 2    34\n",
            "1    33\n",
            "0    33\n",
            "Name: label, dtype: int64\n",
            "검증 레이블 데이터 분포:\n",
            " 1    17\n",
            "0    17\n",
            "2    16\n",
            "Name: label, dtype: int64\n",
            "## 교차 검증: 2\n",
            "학습 레이블 데이터 분포:\n",
            " 1    34\n",
            "2    33\n",
            "0    33\n",
            "Name: label, dtype: int64\n",
            "검증 레이블 데이터 분포:\n",
            " 2    17\n",
            "0    17\n",
            "1    16\n",
            "Name: label, dtype: int64\n",
            "## 교차 검증: 3\n",
            "학습 레이블 데이터 분포:\n",
            " 0    34\n",
            "2    33\n",
            "1    33\n",
            "Name: label, dtype: int64\n",
            "검증 레이블 데이터 분포:\n",
            " 2    17\n",
            "1    17\n",
            "0    16\n",
            "Name: label, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "skf = StratifiedKFold(n_splits=3)\n",
        "n_iter=0\n",
        "\n",
        "for train_index, test_index in skf.split(iris_df, iris_df['label']):\n",
        "    n_iter += 1\n",
        "    label_train= iris_df['label'].iloc[train_index]\n",
        "    label_test= iris_df['label'].iloc[test_index]\n",
        "    print('## 교차 검증: {0}'.format(n_iter))\n",
        "    print('학습 레이블 데이터 분포:\\n', label_train.value_counts())\n",
        "    print('검증 레이블 데이터 분포:\\n', label_test.value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vQJB02lOEyYy",
        "outputId": "75fe5421-16db-4b8e-8cc5-9e550df14621"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "#1 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50\n",
            "#1 검증 세트 인덱스:[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  50\n",
            "  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66 100 101\n",
            " 102 103 104 105 106 107 108 109 110 111 112 113 114 115]\n",
            "\n",
            "#2 교차 검증 정확도 :0.94, 학습 데이터 크기: 100, 검증 데이터 크기: 50\n",
            "#2 검증 세트 인덱스:[ 17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  67\n",
            "  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82 116 117 118\n",
            " 119 120 121 122 123 124 125 126 127 128 129 130 131 132]\n",
            "\n",
            "#3 교차 검증 정확도 :0.98, 학습 데이터 크기: 100, 검증 데이터 크기: 50\n",
            "#3 검증 세트 인덱스:[ 34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  83  84\n",
            "  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99 133 134 135\n",
            " 136 137 138 139 140 141 142 143 144 145 146 147 148 149]\n",
            "\n",
            "## 교차 검증별 정확도: [0.98 0.94 0.98]\n",
            "## 평균 검증 정확도: 0.9666666666666667\n"
          ]
        }
      ],
      "source": [
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "skfold = StratifiedKFold(n_splits=3)\n",
        "n_iter=0\n",
        "cv_accuracy=[]\n",
        "\n",
        "# StratifiedKFold의 split( ) 호출시 반드시 레이블 데이터 셋도 추가 입력 필요  \n",
        "for train_index, test_index  in skfold.split(features, label):\n",
        "    # split( )으로 반환된 인덱스를 이용하여 학습용, 검증용 테스트 데이터 추출\n",
        "    X_train, X_test = features[train_index], features[test_index]\n",
        "    y_train, y_test = label[train_index], label[test_index]\n",
        "    #학습 및 예측 \n",
        "    dt_clf.fit(X_train , y_train)    \n",
        "    pred = dt_clf.predict(X_test)\n",
        "\n",
        "    # 반복 시 마다 정확도 측정 \n",
        "    n_iter += 1\n",
        "    accuracy = np.round(accuracy_score(y_test,pred), 4)\n",
        "    train_size = X_train.shape[0]\n",
        "    test_size = X_test.shape[0]\n",
        "    print('\\n#{0} 교차 검증 정확도 :{1}, 학습 데이터 크기: {2}, 검증 데이터 크기: {3}'\n",
        "          .format(n_iter, accuracy, train_size, test_size))\n",
        "    print('#{0} 검증 세트 인덱스:{1}'.format(n_iter,test_index))\n",
        "    cv_accuracy.append(accuracy)\n",
        "    \n",
        "# 교차 검증별 정확도 및 평균 정확도 계산 \n",
        "print('\\n## 교차 검증별 정확도:', np.round(cv_accuracy, 4))\n",
        "print('## 평균 검증 정확도:', np.mean(cv_accuracy)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KfmfMnQEyYy"
      },
      "source": [
        "cross_val_score( )  \n",
        "교차검증을 수행하려면 폴드세트를 설정하고 설정된 세트의 인덱스를 읽어서 데이터를 분리한 뒤 학습하는 것을 폴드 개수만큼 반복해야 한다. 이를 간편히 한번에 처리하는 함수가 cross_val_score( )다. 교차를 위한 폴드를 나누는 방법은 분류인 경우는 Stratified K 폴드 교차를 회귀인 경우엔  K 폴드 교차를 수행한다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvZMXhYOEyYz",
        "outputId": "2a6ce812-b7a9-4881-f18b-6c37a719ad52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "교차 검증별 정확도: [0.98 0.94 0.98]\n",
            "평균 검증 정확도: 0.9667\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score , cross_validate\n",
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "\n",
        "iris_data = load_iris()\n",
        "dt_clf = DecisionTreeClassifier(random_state=156)\n",
        "\n",
        "data = iris_data.data\n",
        "label = iris_data.target\n",
        "\n",
        "# 성능 지표는 정확도(accuracy) , 교차 검증 세트는 3개 \n",
        "scores = cross_val_score(dt_clf , data , label , scoring='accuracy',cv=3)\n",
        "print('교차 검증별 정확도:',np.round(scores, 4))\n",
        "print('평균 검증 정확도:', np.round(np.mean(scores), 4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keIpOHEsEyY0"
      },
      "source": [
        "교차 검증과 하이퍼파라메터 튜닝을 한 번에 해주는 함수  \n",
        "GridSearchCV()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ8FQ4z2EyY0"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# 데이터를 로딩하고 학습데이타와 테스트 데이터 분리\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris_data.data, iris_data.target, \n",
        "                                                    test_size=0.2, random_state=121)\n",
        "dtree = DecisionTreeClassifier()\n",
        "\n",
        "### parameter 들을 dictionary 형태로 설정\n",
        "parameters = {'max_depth':[1,2,3], 'min_samples_split':[2,3]}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qmf68cMoEyY1",
        "outputId": "10acd6ef-fed0-4547-f977-e6887107bc16"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>params</th>\n",
              "      <th>mean_test_score</th>\n",
              "      <th>rank_test_score</th>\n",
              "      <th>split0_test_score</th>\n",
              "      <th>split1_test_score</th>\n",
              "      <th>split2_test_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>{'max_depth': 1, 'min_samples_split': 2}</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>{'max_depth': 1, 'min_samples_split': 3}</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>5</td>\n",
              "      <td>0.700</td>\n",
              "      <td>0.7</td>\n",
              "      <td>0.70</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>{'max_depth': 2, 'min_samples_split': 2}</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>3</td>\n",
              "      <td>0.925</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>{'max_depth': 2, 'min_samples_split': 3}</td>\n",
              "      <td>0.958333</td>\n",
              "      <td>3</td>\n",
              "      <td>0.925</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>{'max_depth': 3, 'min_samples_split': 2}</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.975</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>{'max_depth': 3, 'min_samples_split': 3}</td>\n",
              "      <td>0.975000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.975</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                     params  mean_test_score  rank_test_score  \\\n",
              "0  {'max_depth': 1, 'min_samples_split': 2}         0.700000                5   \n",
              "1  {'max_depth': 1, 'min_samples_split': 3}         0.700000                5   \n",
              "2  {'max_depth': 2, 'min_samples_split': 2}         0.958333                3   \n",
              "3  {'max_depth': 2, 'min_samples_split': 3}         0.958333                3   \n",
              "4  {'max_depth': 3, 'min_samples_split': 2}         0.975000                1   \n",
              "5  {'max_depth': 3, 'min_samples_split': 3}         0.975000                1   \n",
              "\n",
              "   split0_test_score  split1_test_score  split2_test_score  \n",
              "0              0.700                0.7               0.70  \n",
              "1              0.700                0.7               0.70  \n",
              "2              0.925                1.0               0.95  \n",
              "3              0.925                1.0               0.95  \n",
              "4              0.975                1.0               0.95  \n",
              "5              0.975                1.0               0.95  "
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# param_grid의 하이퍼 파라미터들을 3개의 train, test set fold 로 나누어서 테스트 수행 설정.  \n",
        "### refit=True 가 default 임. True이면 가장 좋은 파라미터 설정으로 재 학습 시킴.  \n",
        "grid_dtree = GridSearchCV(dtree, param_grid=parameters, cv=3, refit=True)\n",
        "\n",
        "# 붓꽃 Train 데이터로 param_grid의 하이퍼 파라미터들을 순차적으로 학습/평가 .\n",
        "grid_dtree.fit(X_train, y_train)\n",
        "\n",
        "# GridSearchCV 결과 추출하여 DataFrame으로 변환\n",
        "scores_df = pd.DataFrame(grid_dtree.cv_results_)\n",
        "scores_df[['params', 'mean_test_score', 'rank_test_score', \\\n",
        "           'split0_test_score', 'split1_test_score', 'split2_test_score']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z05zs7CtEyY1",
        "outputId": "d48ef6ec-4e2a-46d0-dc39-efeb76a51763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GridSearchCV 최적 파라미터: {'max_depth': 3, 'min_samples_split': 2}\n",
            "GridSearchCV 최고 정확도: 0.9750\n"
          ]
        }
      ],
      "source": [
        "print('GridSearchCV 최적 파라미터:', grid_dtree.best_params_)\n",
        "print('GridSearchCV 최고 정확도: {0:.4f}'.format(grid_dtree.best_score_))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SrAMZmBrEyY1",
        "outputId": "0785d175-fa6e-489f-d102-afe8968ffcd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "테스트 데이터 세트 정확도: 0.9667\n"
          ]
        }
      ],
      "source": [
        "# GridSearchCV의 refit으로 이미 학습이 된 estimator 반환\n",
        "estimator = grid_dtree.best_estimator_\n",
        "\n",
        "# GridSearchCV의 best_estimator_는 이미 최적 하이퍼 파라미터로 학습이 됨\n",
        "pred = estimator.predict(X_test)\n",
        "print('테스트 데이터 세트 정확도: {0:.4f}'.format(accuracy_score(y_test,pred)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5ZlLQ58EyY2"
      },
      "source": [
        "<자동 매개변수 검색>  \n",
        "  \n",
        "estimator는 몇몇 하이퍼파라메터에 의해 학습 결과의 차가 커질 수 있다. 최적의 학습을 위해 많은 검증을 통해 최적의 하이퍼파라메터를 직접 찾을 \n",
        "수도 있지만 사이킷 런에서는 자동으로 매개변수의 값을 찾아 주는 API를 제공한다.  \n",
        "  \n",
        "=>  RandomizedSearchCV(estimator:알고리즘객체, n_iter:반복횟수(기본:10),  param_distributions:조정할 파라메터 이름들, random_state)  \n",
        "  \n",
        "최적의 하이퍼 파라메터 값들을 찾아 학습에 바로 적용됨"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ogm26nCQEyY2",
        "outputId": "4e2ff07e-76d3-4d17-df40-c7126a764982"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'max_depth': 9, 'n_estimators': 4}"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from scipy.stats import randint\n",
        "\n",
        "X, y = fetch_california_housing(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
        "\n",
        "# define the parameter space that will be searched over\n",
        "param_distributions = {'n_estimators': randint(1, 5),\n",
        "                       'max_depth': randint(5, 10)}\n",
        "\n",
        "# now create a searchCV object and fit it to the data\n",
        "search = RandomizedSearchCV(estimator=RandomForestRegressor(random_state=0),\n",
        "                            n_iter=5,\n",
        "                            param_distributions=param_distributions,\n",
        "                            random_state=0)\n",
        "search.fit(X_train, y_train)\n",
        "\n",
        "search.best_params_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fbhNXdu5EyY2",
        "outputId": "a980e0e9-c729-4d42-e341-606332f44eb2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.735363411343253"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# the search object now acts like a normal random forest estimator\n",
        "# with max_depth=9 and n_estimators=4\n",
        "search.score(X_test, y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39oNCSlIEyY3"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    },
    "colab": {
      "name": "머신러닝_절차의_이해.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}